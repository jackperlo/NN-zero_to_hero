# ğŸ” Recurrent Neural Network (RNN)

In **Makemore Part 3** (by Andrej Karpathy), the model evolves from a simple MLP into a **Recurrent Neural Network**.  
RNNs are designed to handle **sequential data** by maintaining a *hidden state* that carries information across time steps.  

---

## ğŸ“š Topics Covered

- ğŸ«¥ **Gradient Vanishing** â€“ The challenge where gradients shrink during backpropagation through time, making it hard for RNNs to learn long-term dependencies  
- âš–ï¸ **Kaiming Initialization** â€“ A smart weight initialization strategy that improves training stability in deep networks  
- ğŸ”„ **Batch Normalization** â€“ Normalizing activations within a layer to speed up convergence and stabilize training  

---

âœ¨ RNNs introduce **memory** into the network, enabling it to capture **patterns over sequences**, a stepping stone towards more advanced architectures like **LSTMs** and **Transformers**.
