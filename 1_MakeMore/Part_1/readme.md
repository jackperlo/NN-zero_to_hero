# âœ¨ Makemore

**Makemore** is a simple yet powerful project that demonstrates how language models can be built **from scratch**.  
It starts with a **bigram model**, where the **next token** is predicted based **only on the previous token**.  

While simple, this model lays the groundwork for understanding more advanced architectures like **n-grams, RNNs, and Transformers**.

---

## ðŸ”Ž What is a Bigram Model?

A **bigram model** predicts the next symbol (character or token) based solely on the **current one**.  
For example:  

- "H" â†’ predicts "e"  
- "e" â†’ predicts "l"  
- "l" â†’ predicts "l"  
- "l" â†’ predicts "o"  

This step-by-step process lets us **generate sequences** and understand the **probabilistic structure** of language.

---

## ðŸ“š Topics Covered

- ðŸ”„ **Broadcasting Rule in PyTorch** â€“ Leveraging tensor shapes efficiently  
- ðŸ“Š **Log-Likelihood** â€“ Measuring how well the model fits the data  
- ðŸ§¹ **Model Smoothing** â€“ Handling unseen tokens in training data  
- ðŸŽ­ **One-Hot Encoding** â€“ Representing categorical tokens as vectors  
- ðŸ”¥ **Softmax** â€“ Converting logits into probability distributions  
- ðŸ›¡ **L2 Regularization** â€“ Preventing overfitting by penalizing large weights  

---

## ðŸš€ Why It Matters

âœ¨ With Makemore, you gain a **hands-on understanding** of how models like GPT are built from the ground up!
