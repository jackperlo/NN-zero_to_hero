# ðŸŒ± Micrograd

**Micrograd** is a lightweight **auto-grad engine**.  
The term *autograd* stands for **automatic differentiation**, which enables **backpropagation**.

---

## ðŸ”„ What is Backpropagation?

Backpropagation is an algorithm that efficiently computes the **gradient of a loss function** with respect to the **weights of a neural network**.  

This allows us to **iteratively update the weights** in order to **minimize the loss function**, which in turn **improves the accuracy of the model**.  

In simple terms:  
ðŸ‘‰ Backpropagation is the **recursive application of the chain rule** *backwards* through the computational graph.

---

## ðŸ“š Topics Covered

- ðŸ“Œ Local Derivative Computation  
- ðŸ“Œ Chain Rule for Derivative Computation  
- ðŸ“Œ Manual Backward Propagation  
- ðŸ“Œ Loss Function (MSE)  
- ðŸ“Œ Gradient Descent Step  
- ðŸ“Œ Training Batch Size Parameter 
- ðŸ“Œ Multi Layer Perceptron (MLP), simplest possible

---
âœ¨ With these building blocks, we can construct and train simple neural networks from scratch!
