# ðŸŒ± Micrograd

**Micrograd** is a lightweight **auto-grad engine**.  
The term *autograd* stands for **automatic differentiation**, which enables **backpropagation**.

---

## ðŸ”„ What is Backpropagation?

Backpropagation is an algorithm that efficiently computes the **gradient of a loss function** with respect to the **weights of a neural network**.  

This allows us to **iteratively update the weights** in order to **minimize the loss function**, which in turn **improves the accuracy of the model**.  

In simple terms:  
ðŸ‘‰ Backpropagation is the **recursive application of the chain rule** *backwards* through the computational graph.

---

## ðŸ“š Topics Covered

- ðŸ§® **Local Derivative Computation** â€“ Finding gradients of simple operations  
- ðŸ”— **Chain Rule for Derivative Computation** â€“ Combining local gradients across a graph  
- ðŸ”„ **Manual Backward Propagation** â€“ Explicitly applying derivatives step by step  
- ðŸ“Š **Loss Function (MSE)** â€“ Measuring prediction error with *Mean Squared Error*  
- ðŸ“‰ **Gradient Descent Step** â€“ Updating parameters to reduce loss  
- ðŸ“¦ **Training Batch Size Parameter** â€“ Controlling how much data is processed per step  
- ðŸ§  **Multi-Layer Perceptron (MLP)** â€“ The simplest neural network built from Micrograd  

---

âœ¨ With these building blocks, we can construct and train **simple neural networks from scratch**!
