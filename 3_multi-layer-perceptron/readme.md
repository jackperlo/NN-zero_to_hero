# ðŸ§  Multi-Layer Perceptron (MLP)

We extend the **Makemore bigram model** by using **3-character context** to predict the **next character**.  
This marks the transition from simple statistical models to **neural networks** capable of capturing richer dependencies.  

---

## ðŸ“š Topics Covered

- ðŸ”¢ **2D Embedding** â€“ Representing characters as low-dimensional learnable vectors instead of sparse one-hot encodings  
- ðŸ“‰ **Learning Rate Decay** â€“ Gradually reducing the learning rate during training for more stable convergence  

---

âœ¨ With MLPs, we move beyond bigrams and start leveraging **neural architectures** that can model **non-linear patterns** in data!
