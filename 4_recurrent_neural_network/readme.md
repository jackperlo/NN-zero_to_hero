# 🔁 Recurrent Neural Network (RNN)

In **Makemore Part 3** (by Andrej Karpathy), the model evolves from a simple MLP into a **Recurrent Neural Network**.  
RNNs are designed to handle **sequential data** by maintaining a *hidden state* that carries information across time steps.  

---

## 📚 Topics Covered

- 🫥 **Gradient Vanishing** – The challenge where gradients shrink during backpropagation through time, making it hard for RNNs to learn long-term dependencies  
- ⚖️ **Kaiming Initialization** – A smart weight initialization strategy that improves training stability in deep networks  
- 🔄 **Batch Normalization** – Normalizing activations within a layer to speed up convergence and stabilize training  

---

✨ RNNs introduce **memory** into the network, enabling it to capture **patterns over sequences**, a stepping stone towards more advanced architectures like **LSTMs** and **Transformers**.
